# ðŸ¤– Machine Learning Basics

Machine Learning is not just a "black box" with a button. It is a discipline at the intersection of statistics, mathematics, and programming. To become a professional, you need to understand what's happening "under the hood."

---

## ðŸ“‘ Table of Contents
1. [Glossary of Terms](#glossary-of-terms)
2. [The Three Pillars: Data, Features, Algorithm](#three-pillars)
3. [Types of Learning and Tasks](#learning-types)
4. [Life Cycle of an ML Project](#life-cycle)
5. [Data Preprocessing](#preprocessing)
6. [Basic Algorithms (Non-Neural)](#algorithms)
7. [Performance Evaluation and Validation](#evaluation)

---

## ðŸ“– Glossary of Terms
First, let's understand the language ML engineers speak:

*   **Model**: A mathematical representation of a real-world process. An algorithm "charged" with data.
*   **Dataset**: A collection of data.
*   **Features (X)**: Input parameters (e.g., house area, number of rooms).
*   **Target Variable (y)**: What we are predicting (e.g., house price).
*   **Weights (w)**: Coefficients that the model adjusts during training (the degree of a feature's influence).
*   **Bias (b)**: A constant that allows the model to be more flexible.
*   **Training**: The process of adjusting weights to minimize error.
*   **Hyperparameters**: Model settings defined by a human **before** training begins (e.g., tree depth or learning rate).

---

## ðŸŽ“ Types of Learning

### 1. Supervised Learning ðŸ‘¨â€ðŸ«
We know the correct answers. The model learns to map X to y.
- **Classification**: Discrete answers (Yes/No, Cat/Dog).
- **Regression**: Continuous numbers (Price, Temperature).

### 2. Unsupervised Learning ðŸ”
There are no labels/answers. The model looks for hidden structures.
- **Clustering**: Grouping data points.
- **Dimensionality Reduction (PCA)**: Simplifying data without losing much meaning.

### 3. Reinforcement Learning ðŸŽ®
Agent -> Environment -> Reward. Learning from own experience in a dynamic environment.

---

## ðŸ› ï¸ Data Preprocessing (The "Magic" Part)
80% of an ML engineer's work is data preparation. An algorithm won't "eat" raw text or images without processing.

1.  **Imputation**: What to do if data is missing? (Mean, median, or row deletion).
2.  **Scaling/Normalization**: 
    - Algorithms (especially KNN and neural networks) are sensitive to scale. If one feature is in dollars (millions) and another is in meters (dozens), the model will "listen" only to the dollars.
    - **StandardScaler**: Adjusts to mean 0 and standard deviation 1.
    - **MinMaxScaler**: Squeezes values into the range [0, 1].
3.  **Encoding**: Converting text to numbers.
    - **One-Hot Encoding**: Each category becomes a separate column (0 or 1).
    - **Label Encoding**: Categories are turned into ordinal numbers (0, 1, 2...).

---

## ðŸ§¬ Basic Classical Algorithms

### Linear Models
- **Linear Regression**: Finds the straight line that best describes the data.
- **Logistic Regression**: Despite the name, this is a **classification** method. It uses a sigmoid function to predict class probability.

### Trees and Ensembles
- **Decision Tree**: A set of "if-then" conditions. Very human-readable but prone to overfitting.
- **Random Forest**: A "forest" of many trees. The prediction is a majority vote. A very powerful and stable method.
- **Gradient Boosting (XGBoost, CatBoost, LightGBM)**: Sequential building of trees, where each new one corrects the errors of the previous ones. **SOTA** (State of the art) for tabular data.

### Metric-based and Others
- **KNN (K-Nearest Neighbors)**: "Tell me who your neighbors are, and I'll tell you who you are." Solves the problem based on the closest points.
- **SVM (Support Vector Machine)**: Looks for a hyperplane that best separates the classes.

---

## âš–ï¸ Overfitting vs Underfitting

| Concept | Briefly | What to do? |
| :--- | :--- | :--- |
| **Overfitting** | "Memorized" answers, doesn't understand the essence. | Regularization (L1, L2), more data, Dropout. |
| **Underfitting** | The model is too simple for this data. | Complicate the model, add new features. |

> [!IMPORTANT]
> **Bias-Variance Tradeoff**: This is the balance between error due to overly simple assumptions (Bias) and error due to excessive sensitivity to noise in the data (Variance).

---

## ðŸ“ Validation and Metrics

### Cross-Validation (K-Fold)
Splitting data into 5-10 parts, where on each part we manage to both learn and test. This gives the most honest quality assessment.

### Confusion Matrix
Allows you to see not just "accuracy," but WHERE exactly the model is failing:
- **True Positive (TP)**: Said "cancer," and it really is cancer.
- **False Positive (FP)**: Said "cancer," but the person is healthy (Type I error).
- **False Negative (FN)**: Said "healthy," but the person has cancer (Type II error - the most dangerous!).
- **True Negative (TN)**: Said "healthy," and they are healthy.

---

---

> [!TIP]
> **For beginners**: Start with linear regression and random forest. They forgive many mistakes.
> **For pros**: Never neglect EDA (Exploratory Data Analysis). No XGBoost will save you from "garbage" at the input. ðŸš®
