# ğŸš€ Deep Learning

Deep Learning is a subset of machine learning that focuses on architectures with a large number of layers. This is where the most significant breakthroughs of recent years are happening.

---

## ğŸ“‘ Table of Contents
1. [CNN: Computer Vision](#cnn)
2. [RNN and GRU: Working with Sequences](#rnn)
3. [Transformers: Attention Is All You Need](#transformers)
4. [Attention Mechanism: How does it work?](#attention)
5. [Generative Models (GAN, Diffusion)](#generative)
6. [Fine-tuning and Transfer Learning](#transfer-learning)

---

## ğŸ–¼ï¸ CNN: Convolutional Neural Networks
Specialized for data with a grid-like structure (images).

### Anatomy of a Convolution:
- **Filters (Kernels)**: Small matrices (e.g., 3x3) that "slide" over the image.
- **Stride**: The number of pixels the filter shifts. Large stride = smaller output image.
- **Padding**: Adding zeros around the edges to preserve image size after convolution.
- **Receptive Field**: The area of the input image that a specific neuron in a deep layer "sees."

**Layers:** `Conv -> ReLU -> Pooling (Max/Avg) -> ... -> Fully Connected`.

---

## ğŸ” RNN: Recurrent Networks and GRU
Used for sequences (text, audio, time series).

### The Long-Term Memory Problem
Standard RNNs suffer from the **Vanishing Gradient** problem. They forget the beginning of long sentences.

### Solution: GRU (Gated Recurrent Unit)
A simplified and faster version of LSTM. It uses two types of gates:
1.  **Update Gate**: How much old information to carry forward through time.
2.  **Reset Gate**: Which part of the past to forget right now.

---

## âš¡ Transformers: A New Era (GPT, BERT)
Transformers abandoned sequential processing (like in RNNs) in favor of parallel processing.

### Why are they so powerful?
RNN reads words in order: `I` -> `love` -> `Go`.
The Transformer sees the **entire sentence at once**. This allows it to understand relationships between distant words.

### Positional Encoding
Since a Transformer sees everything at once, it doesn't know the word order. To help it understand that "The dog ate the food" and "The food ate the dog" are different, special vectors (sinusoids) are added to the words to encode their position.

---

## ğŸ‘ï¸ Attention Mechanism
A mechanism that allows the model to focus on the most important parts.

In **Multi-Head Attention**, the model runs several "attention heads" in parallel. One head might focus on grammar, another on meaning, and a third on relationships between names.

**Formula:** $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$
- **Query (Q)**: What I am looking for.
- **Key (K)**: My characteristics.
- **Value (V)**: My meaning.

---

## ğŸ¨ Generative Models
Models that don't just classify but create something new.

1.  **GAN (Generative Adversarial Networks)**: A battle between two networks. The **Generator** tries to create a fake, and the **Discriminator** tries to distinguish it from reality. Through this struggle, both become experts.
2.  **Diffusion Models (Midjourney, DALL-E)**: The idea is to first turn an image into noise and then teach the neural network to reconstruct the image from that noise.

---

## ğŸ”„ Transfer Learning: The "Don't Reinvent the Wheel" Philosophy
In 2024, nobody trains huge networks from scratch.

1. Take a **Pre-trained model** (ResNet for CV, Llama/GPT for NLP).
2. Perform **Fine-tuning**: Further train the model on your own small dataset.
3. This saves millions of dollars in electricity and weeks of time.

---

| Architecture | When to use? | Example |
| :--- | :--- | :--- |
| **CNN** | Images, video | Face recognition |
| **RNN/GRU** | Text, sound, stocks | Translator |
| **Transformer** | Text (long), Code | ChatGPT, GitHub Copilot |
| **YOLO** | Object detection | Autonomous vehicles |

---

---

> [!TIP]
> Success in Deep Learning is 50% data, 30% architecture, and 20% the ability to correctly tune hyperparameters (Fine-tuning). ğŸš€
