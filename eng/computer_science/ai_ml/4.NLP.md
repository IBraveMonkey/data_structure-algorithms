# üí¨ NLP: Natural Language Processing

Natural Language Processing (NLP) has come a long way from simple word counting to models capable of holding philosophical conversations.

---

## üìë Table of Contents
1. [Vectorization and Tokenization](#basic)
2. [Word Embeddings (Semantic Vectors)](#embeddings)
3. [The Era of Transformers (BERT, GPT)](#transformers)
4. [RAG: How to make AI smarter and more accurate](#rag)
5. [Prompt Engineering](#prompts)

---

## üî§ Vectorization and Tokenization

### Tokenization
Before converting a word into a number, it must be broken down into parts (tokens).
- **Word-based**: One word = one token. Problem: "run," "running," "runner" are treated as completely different tokens.
- **Subword-based (BPE, WordPiece)**: Cuts words into pieces (e.g., `run` + `ning`). This allows the model to understand new words assembled from familiar parts. Used in all modern LLMs.

---

## üß¨ Word Embeddings

### Static (Word2Vec, GloVe)
Each word has a single fixed vector. "Bark" (tree) and "Bark" (dog) would have the same vector ‚Äî which is problematic.

### Contextual (ELMo, BERT)
The word vector changes depending on its surroundings.
**ELMo** (2018) proposed using bidirectional LSTMs so that the vector for the word "bank" in "bank of a river" and "commercial bank" would be different.

---

## ‚ö° The Era of Transformers

### BERT vs GPT
- **BERT**: Understands context (Encoder-based). Ideal for classification, search, and data extraction.
- **GPT**: Generates text (Decoder-based). Ideal for chats and creative writing.

---

## üìö RAG (Retrieval-Augmented Generation)
**RAG** is the #1 technology for business in 2024.

**The Problem**: LLMs are limited by the knowledge they were trained on (cutoff date) and can hallucinate.
**The Solution**:
1. The user asks a question.
2. The system searches your knowledge base (PDFs, Wikis, DBs) for relevant text chunks.
3. The system feeds these chunks to the LLM along with the question: "Using this text, answer the question."

> [!TIP]
> RAG turns a model from a "student who memorized everything" into a "professor with an open book."

---

## ‚úçÔ∏è Prompt Engineering
The skill of effectively communicating with the model.

1.  **Zero-Shot**: "Translate this to English: ..."
2.  **Few-Shot**: Giving the model 2-3 examples before the task.
3.  **Chain of Thought**: "Think step by step" (forces the model to reason logically before giving an answer).

---

## ü§ñ Working with LLMs: Parameters and Inference

When you run a model (e.g., via Ollama or an API), you encounter several parameters. Here is what they mean:

### 1. Model Size: What is "3B", "7B", "70B"?
The letter **B (Billions)** stands for billions of parameters (weights) inside the neural network.
- **1.5B - 3B**: Small models (Qwen, Phi-3). They run fast even on weak PCs and phones. Good for simple tasks.
- **7B - 14B**: The "Gold Standard." Smart, fast, and fit into an average GPU (8-12 GB VRAM).
- **70B+**: Extremely smart (Llama 3, GPT-4). Require server-grade hardware or multiple powerful graphics cards.

> [!NOTE]
> **Quantization**: This is weight compression. For example, the `Q4_K_M` (4-bit) format allows running a 7B model on 8 GB of RAM instead of 28 GB, with almost no loss in quality.

---

### 2. Inference Parameters

#### **Temperature** üå°Ô∏è
The level of "creativity" or randomness.
- **0.0 - 0.2**: Strict, logical answers. Ideal for code and facts.
- **0.7 - 1.0**: A balance. Good for general conversation.
- **1.2+**: High risk of "gibberish" and very strange ideas.

#### **Top-P (Nucleus Sampling)**
The model chooses words from those whose cumulative probability adds up to P (e.g., 0.9). Helps filter out improbable "garbage" while leaving only meaningful options.

#### **Top-K**
Limits the selection to only the K most probable words (e.g., Top-K = 40). This makes the text more focused.

---

### 3. Context Window
This is the model's "working memory" within a single dialogue.
- **8k - 32k**: Standard. Enough for a long chat.
- **128k - 1M+**: (Gemini, Claude). You can feed it an entire book or a project's entire codebase.

**Important**: The more context you provide, the slower the generation becomes and the more VRAM the model consumes.

---

## üìä NLP Tasks

| Task | Solution | Tool |
| :--- | :--- | :--- |
| **Sentiment Analysis** | BERT / RoBERTa | Hugging Face |
| **NER (Named Entity Recognition)**| SpaCy | `en_core_web_sm` |
| **Summarization** | BART / T5 | Transformers |
| **Search (Semantic)** | Sentence-BERT | FAISS / Pinecone |

---

---

> [!CAUTION]
> **Hallucinations**: Always remember that an LLM is a statistical predictor of the next word. It doesn't "know" facts the way a database does. Use RAG to minimize errors. ‚ö†Ô∏è
