# ğŸ§  Neural Networks

Neural networks are the heart of modern AI. In this chapter, we will break down not only "what they are" but also "how they work mathematically" so that you can debug complex models.

---

## ğŸ“‘ Table of Contents
1. [Anatomy of a Neuron](#neuron-anatomy)
2. [Activation Functions: Why do we need them?](#activation-functions)
3. [Loss Functions](#loss-functions)
4. [Training: Gradient Descent and Optimizers](#optimization)
5. [Backpropagation (The Chain Rule)](#backpropagation)
6. [Regularization and Overfitting Prevention](#regularization)
7. [Initialization and Normalization](#normalization)

---

## âš¡ Anatomy of a Neuron
A neuron is simply a summer with weighted inputs.

**Formula:** $z = \sum (x_i \cdot w_i) + b$
Then $y = \sigma(z)$, where $\sigma$ is the activation function.

*   **Weights (w)**: Parameters that the network "tunes" to fit the answer.
*   **Bias (b)**: Allows shifting the function's graph left or right. This is necessary for a neuron to activate even if all inputs are zero.

---

## ğŸ“ˆ Activation Functions
Without them, a network turns into one giant linear model. Activation introduces **non-linearity**, allowing the network to learn complex dependencies.

| Name | Formula | Features | Where to use? |
| :--- | :--- | :--- | :--- |
| **ReLU** | $max(0, x)$ | Fast, solves the vanishing gradient problem. | Hidden layers (standard). |
| **Sigmoid** | $1 / (1 + e^{-x})$ | Output [0, 1]. Vanishes for large $x$. | Binary classification. |
| **Softmax** | $e^{x_i} / \sum e^{x_j}$ | Sum of outputs = 1. | Last layer (multi-class). |
| **Leaky ReLU**| $max(0.01x, x)$ | Prevents neurons from "dying." | If standard ReLU isn't working well. |

---

## ğŸ“‰ Loss Functions
Error is a "compass" for the model. It shows how far we are from the truth.

1.  **MSE (Mean Squared Error)**: Mean of squared differences. Used in **regression**. Heavily penalizes large errors.
2.  **Binary Cross-Entropy (Log Loss)**: Used for **binary classification**. Works well for predicting probabilities.
3.  **Categorical Cross-Entropy**: For multi-class classification.

---

## ğŸ”„ Training: Backpropagation and Optimizers

Training is the search for weights where the Loss is minimized.

### Backpropagation
Based on the **Chain Rule** from calculus. We calculate the derivative of the error with respect to each weight:
$$\frac{\partial Loss}{\partial w} = \frac{\partial Loss}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$$
This helps us understand: "If I change this specific weight by 0.001, how will the final error change?".

### Optimizers (Gradient Descent Variants)
- **SGD (Stochastic Gradient Descent)**: Basic descent. Fast but noisy.
- **Momentum**: Adds "inertia." If we keep moving in one direction, we accelerate. Helps bypass local minima.
- **Adam (Adaptive Moment Estimation)**: The king of optimizers. Combines Momentum ideas with adaptive Learning Rate for each weight. Almost always a good starting point.

---

## ğŸ›¡ï¸ Regularization: Fighting Overfitting

1.  **Dropout**: During training, we randomly "turn off" (zero out) a portion of neurons (e.g., 20%). This prevents the network from relying on specific connections and makes it more robust.
2.  **L1 / L2 Regularization**: We add a penalty to the Loss function for weights being too large. This forces weights to remain small and simple.
3.  **Early Stopping**: Stop training when the error on the validation set begins to rise while the training error is still falling.

---

## ğŸ—ï¸ Normalization and Initialization

### Batch Normalization
Scaling data **inside** the network between layers.
- Speeds up training significantly.
- Allows for a larger Learning Rate.
- Acts as light regularization.

### Weight Initialization
If all weights are set to zero, the network won't learn.
- **Xavier/Glorot**: For Sigmoid/Tanh.
- **He Initialization**: Specifically for **ReLU** (modern standard).

---

> [!IMPORTANT]
> **Vanishing Gradient**: In very deep networks using Sigmoid, the gradient becomes smaller and smaller through multiplication (Chain Rule) until it turns into 0. As a result, the early layers don't learn at all. Solution â€” **ReLU** and **Batch Norm**.

---

---

> [!TIP]
> If your model is learning poorly:
> 1. Check the Learning Rate (try 1e-3).
> 2. Add Batch Normalization.
> 3. Try a simpler or more complex architecture depending on your data size. ğŸ› ï¸
