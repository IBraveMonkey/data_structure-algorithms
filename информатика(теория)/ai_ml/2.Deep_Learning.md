# üöÄ Deep Learning (–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ)

## üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–ß—Ç–æ —Ç–∞–∫–æ–µ Deep Learning?](#—á—Ç–æ-—Ç–∞–∫–æ–µ-deep-learning)
2. [CNN: –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏](#cnn-—Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ-—Å–µ—Ç–∏)
3. [RNN –∏ LSTM: –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏](#rnn-–∏-lstm)
4. [Transformers: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ AI](#transformers)
5. [Attention Mechanism (–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è)](#attention-mechanism)
6. [Transfer Learning (–ü–µ—Ä–µ–Ω–æ—Å –æ–±—É—á–µ–Ω–∏—è)](#transfer-learning)

---

## ‚ùì –ß—Ç–æ —Ç–∞–∫–æ–µ Deep Learning?

**Deep Learning** ‚Äî —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö (—Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–µ–≤). –ò–º–µ–Ω–Ω–æ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –¥–æ—Å—Ç–∏—á—å –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–µ—á–∏, –ø–µ—Ä–µ–≤–æ–¥–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. üåä

> [!NOTE]
> "–ì–ª—É–±–æ–∫–∞—è" —Å–µ—Ç—å ‚Äî —ç—Ç–æ –æ–±—ã—á–Ω–æ —Å–µ—Ç—å —Å **–±–æ–ª–µ–µ —á–µ–º 3 —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏**. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Å–æ—Ç–Ω–∏ —Å–ª–æ–µ–≤ (ResNet-152, GPT-3 —Å 96 —Å–ª–æ—è–º–∏ Transformer).

---

## üñºÔ∏è CNN: –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Convolutional Neural Networks)

**CNN** ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ

 —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å **–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏**. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏—é —Å–≤–µ—Ä—Ç–∫–∏ (convolution), —á—Ç–æ–±—ã –Ω–∞—Ö–æ–¥–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫—Ä–∞—è, —Ç–µ–∫—Å—Ç—É—Ä—ã, —Ñ–æ—Ä–º—ã).

### –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–∏:

1.  **Convolutional Layer (–°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π)**:
    - –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã (—è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏) –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
    - –ö–∞–∂–¥—ã–π —Ñ–∏–ª—å—Ç—Ä –∏—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏).
    
2.  **Pooling Layer (–°–ª–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏)**:
    - –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–æ–±—ã—á–Ω–æ MaxPooling).
    - –ë–µ—Ä–µ—Ç –º–∞–∫—Å–∏–º—É–º –∏–∑ –æ–∫–Ω–∞ 2√ó2, —É–º–µ–Ω—å—à–∞—è –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ 4 —Ä–∞–∑–∞.
    
3.  **Fully Connected Layer (–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π)**:
    - –û–±—ã—á–Ω—ã–π —Å–ª–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –≤ –∫–æ–Ω—Ü–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

```mermaid
graph LR
    I[Image 224x224] --> C1[Conv + ReLU]
    C1 --> P1[MaxPool]
    P1 --> C2[Conv + ReLU]
    C2 --> P2[MaxPool]
    P2 --> FC[Fully Connected]
    FC --> Out[Softmax: Class]
    
    style I fill:#9cf
    style Out fill:#f9c
```

### –ü–æ—á–µ–º—É CNN —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π?
- **–õ–æ–∫–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏**: –ù–µ–π—Ä–æ–Ω —Å–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π —É—á–∞—Å—Ç–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ).
- **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤**: –û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ–∏–ª—å—Ç—Ä –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –≤—Å–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é ‚Üí –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- **–ò–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**: –ü–µ—Ä–≤—ã–µ —Å–ª–æ–∏ –≤–∏–¥—è—Ç –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–ª–∏–Ω–∏–∏), –ø–æ—Å–ª–µ–¥–Ω–∏–µ ‚Äî —Å–ª–æ–∂–Ω—ã–µ (–ª–∏—Ü–∞, –æ–±—ä–µ–∫—Ç—ã).

---

## üîÅ RNN –∏ LSTM: –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏

**RNN (Recurrent Neural Networks)** ‚Äî —ç—Ç–æ —Å–µ—Ç–∏ –¥–ª—è **–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π** (—Ç–µ–∫—Å—Ç, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã, –∞—É–¥–∏–æ). –û–Ω–∏ –∏–º–µ—é—Ç "–ø–∞–º—è—Ç—å": –≤—ã—Ö–æ–¥ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —à–∞–≥–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–µ–∫—É—â–∏–π.

```mermaid
graph LR
    X1[Word 1] --> RNN1[RNN Cell]
    RNN1 --> H1[Hidden State]
    H1 --> RNN2[RNN Cell]
    X2[Word 2] --> RNN2
    RNN2 --> H2[Hidden State]
    H2 --> RNN3[RNN Cell]
    X3[Word 3] --> RNN3
```

### –ü—Ä–æ–±–ª–µ–º–∞ –æ–±—ã—á–Ω—ã—Ö RNN:
**Vanishing Gradient** (–ó–∞—Ç—É—Ö–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç): –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã–º, –∏ —Å–µ—Ç—å "–∑–∞–±—ã–≤–∞–µ—Ç" –Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

---

### LSTM (Long Short-Term Memory) üß†
–≠—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RNN, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ **–¥–æ–ª–≥–∏–π —Å—Ä–æ–∫**.

**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
- **Forget Gate** (–í—Ä–∞—Ç–∞ –∑–∞–±—ã–≤–∞–Ω–∏—è): –ß—Ç–æ —É–¥–∞–ª–∏—Ç—å –∏–∑ –ø–∞–º—è—Ç–∏.
- **Input Gate** (–í—Ä–∞—Ç–∞ –≤–≤–æ–¥–∞): –ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –ø–∞–º—è—Ç—å.
- **Output Gate** (–í—Ä–∞—Ç–∞ –≤—ã–≤–æ–¥–∞): –ß—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –¥–∞–ª—å—à–µ.

```mermaid
graph TD
    Ct1[Cell State t-1] --> FG[Forget Gate]
    Xt[Input Xt] --> FG
    Ht1[Hidden t-1] --> FG
    
    FG --> Ct[Cell State t]
    IG[Input Gate] --> Ct
    Ct --> OG[Output Gate]
    OG --> Ht[Hidden t]
```

**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**: –ü–µ—Ä–µ–≤–æ–¥, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π, –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.

---

## ‚ö° Transformers: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ AI

**Transformer** (2017, Google) ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è **–ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–∏–ª–∞** RNN/LSTM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∞ –ª–µ–∂–∏—Ç –≤ –æ—Å–Ω–æ–≤–µ GPT, BERT, ChatGPT. üî•

### –ì–ª–∞–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN:
- **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º**: RNN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –ø–æ –æ—á–µ—Ä–µ–¥–∏. Transformer –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç **–≤—Å–µ —Å–ª–æ–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ**.
- **Self-Attention**: –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –∏ —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –≤–∞–∂–Ω—ã –¥–ª—è –µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è.

```mermaid
graph TB
    Input[–í—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ] --> EMB[Embedding + Positional Encoding]
    EMB --> ENC[Encoder: Multi-Head Attention]
    ENC --> FF[Feed Forward]
    FF --> DEC[Decoder: Masked Attention]
    DEC --> OUT[–í—ã—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ]
```

### Encoder-Decoder —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
- **Encoder**: –ü–æ–Ω–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.
- **Decoder**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–µ–≤–æ–¥, –æ—Ç–≤–µ—Ç).

---

## üëÅÔ∏è Attention Mechanism (–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è)

**Attention** ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–± —Å–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª–∏: "–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞".

**–ü—Ä–∏–º–µ—Ä**: –ü—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Ñ—Ä–∞–∑—ã "–Ø –ª—é–±–ª—é –∫–æ—à–µ–∫" –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç —Å–ª–æ–≤–æ "–∫–æ—à–µ–∫", –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ–±—Ä–∞—Ç–∏—Ç—å **–±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è** –Ω–∞ —Å–ª–æ–≤–æ "cats", –∞ –Ω–µ –Ω–∞ "I" –∏–ª–∏ "love".

### Self-Attention –≤ Transformers:
–ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—ã—á–∏—Å–ª—è–µ—Ç **–≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è** –¥–ª—è –≤—Å–µ—Ö –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤:
```
Attention(Q, K, V) = softmax(Q¬∑K^T / ‚àöd) ¬∑ V
```
- **Q (Query)**: "–ß—Ç–æ —è –∏—â—É?"
- **K (Key)**: "–ß—Ç–æ —É –º–µ–Ω—è –µ—Å—Ç—å?"
- **V (Value)**: "–ö–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–µ—Ä–µ–¥–∞—Ç—å?"

---

## üîÑ Transfer Learning (–ü–µ—Ä–µ–Ω–æ—Å –æ–±—É—á–µ–Ω–∏—è)

–ò–¥–µ—è: –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è, –±–µ—Ä–µ–º **–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é** –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ ImageNet —Å 14 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫) –∏ **–¥–æ–æ–±—É—á–∞–µ–º** –Ω–∞ —Å–≤–æ–µ–π –∑–∞–¥–∞—á–µ.

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- **–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö**: –í–∞–º –Ω–µ –Ω—É–∂–Ω—ã –º–∏–ª–ª–∏–æ–Ω—ã –ø—Ä–∏–º–µ—Ä–æ–≤.
- **–ë—ã—Å—Ç—Ä–µ–µ**: –û–±—É—á–µ–Ω–∏–µ –∑–∞–Ω–∏–º–∞–µ—Ç —á–∞—Å—ã, –∞ –Ω–µ –Ω–µ–¥–µ–ª–∏.
- **–õ—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ**: –ú–æ–¥–µ–ª—å —É–∂–µ –∑–Ω–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.

### –¢–∏–ø–∏—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. –í–∑—è—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (BERT, GPT, ResNet).
2. **–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å** –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏ (–æ–Ω–∏ —É–∂–µ –∑–Ω–∞—é—Ç –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏).
3. **–î–æ–æ–±—É—á–∏—Ç—å** —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ –Ω–∞ –≤–∞—à–µ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ.

```mermaid
graph LR
    PM[–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: ImageNet] --> FL[–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —Å–ª–æ–∏ 1-10]
    FL --> FT[–î–æ–æ–±—É—á–∏—Ç—å —Å–ª–æ–∏ 11-12]
    FT --> YT[–í–∞—à–∞ –∑–∞–¥–∞—á–∞: –∫–æ—à–∫–∏ vs —Å–æ–±–∞–∫–∏]
```

> [!TIP]
> **Fine-tuning** ‚Äî —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ 2024 –≥–æ–¥—É. –û–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è –∏–º–µ–µ—Ç —Å–º—ã—Å–ª —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏:
> - –£ –≤–∞—Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Å–Ω–∏–º–∫–∏).
> - –£ –≤–∞—Å –æ–≥—Ä–æ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (–∫–∞–∫ —É Google/OpenAI).

---

## üåü –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

| –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ | –ó–∞–¥–∞—á–∞ | –ì–æ–¥ | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å |
| :--- | :--- | :--- | :--- |
| **LeNet** | –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ü–∏—Ñ—Ä | 1998 | –ü–µ—Ä–≤–∞—è CNN |
| **AlexNet** | –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ImageNet | 2012 | –ü—Ä–æ—Ä—ã–≤ –≤ CV |
| **ResNet** | –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | 2015 | Skip connections (–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏) |
| **LSTM** | –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ | 1997 | –†–µ—à–∞–µ—Ç vanishing gradient |
| **BERT** | NLP (–ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞) | 2018 | Bidirectional Transformer |
| **GPT-3/4** | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ | 2020/2023 | –û–≥—Ä–æ–º–Ω—ã–π (175B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) |
| **YOLO** | –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ | 2015 | –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è |

---

> [!IMPORTANT]
> –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –æ–≥—Ä–æ–º–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ **GPU** (NVIDIA) –∏–ª–∏ **TPU** (Google). –û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è GPT-3 —Å—Ç–æ–∏—Ç –º–∏–ª–ª–∏–æ–Ω—ã –¥–æ–ª–ª–∞—Ä–æ–≤. –î–ª—è –ª–∏—á–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transfer Learning –∏ –æ–±–ª–∞—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã (Google Colab, AWS). üí∞‚ö°
