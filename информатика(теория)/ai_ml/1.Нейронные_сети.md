# üß† –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Neural Networks)

–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ ‚Äî —ç—Ç–æ —Å–µ—Ä–¥—Ü–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ò–ò. –í —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã —Ä–∞–∑–±–µ—Ä–µ–º –Ω–µ —Ç–æ–ª—å–∫–æ "—á—Ç–æ —ç—Ç–æ", –Ω–æ –∏ "–∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏", —á—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏.

---

## üìë –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–ê–Ω–∞—Ç–æ–º–∏—è –Ω–µ–π—Ä–æ–Ω–∞](#–∞–Ω–∞—Ç–æ–º–∏—è-–Ω–µ–π—Ä–æ–Ω–∞)
2. [–§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: –ó–∞—á–µ–º –æ–Ω–∏ –Ω—É–∂–Ω—ã?](#—Ñ—É–Ω–∫—Ü–∏–∏-–∞–∫—Ç–∏–≤–∞—Ü–∏–∏)
3. [–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Loss Functions)](#loss-functions)
4. [–û–±—É—á–µ–Ω–∏–µ: –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
5. [Backpropagation (–¶–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ)](#backpropagation)
6. [–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è](#—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
7. [–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è](#–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)

---

## ‚ö° –ê–Ω–∞—Ç–æ–º–∏—è –Ω–µ–π—Ä–æ–Ω–∞
–ù–µ–π—Ä–æ–Ω ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—É–º–º–∞—Ç–æ—Ä —Å–æ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–∞–º–∏.

**–§–æ—Ä–º—É–ª–∞:** $z = \sum (x_i \cdot w_i) + b$
–ó–∞—Ç–µ–º $y = \sigma(z)$, –≥–¥–µ $\sigma$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.

*   **–í–µ—Å–∞ (Weights)**: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ç—å "–∫—Ä—É—Ç–∏—Ç", —á—Ç–æ–±—ã –ø–æ–¥–æ–≥–Ω–∞—Ç—å –æ—Ç–≤–µ—Ç.
*   **–°–º–µ—â–µ–Ω–∏–µ (Bias)**: –ü–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–≤–∏–≥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–ª–µ–≤–æ-–≤–ø—Ä–∞–≤–æ. –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –Ω–µ–π—Ä–æ–Ω –º–æ–≥ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å—Å—è, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—Å–µ –≤—Ö–æ–¥—ã —Ä–∞–≤–Ω—ã –Ω—É–ª—é.

---

## üìà –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
–ë–µ–∑ –Ω–∏—Ö —Å–µ—Ç—å –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –æ–¥–Ω—É –æ–≥—Ä–æ–º–Ω—É—é –ª–∏–Ω–µ–π–Ω—É—é –º–æ–¥–µ–ª—å. –ê–∫—Ç–∏–≤–∞—Ü–∏—è –≤–Ω–æ—Å–∏—Ç **–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å**, –ø–æ–∑–≤–æ–ª—è—è —Å–µ—Ç–∏ –≤—ã—É—á–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.

| –ù–∞–∑–≤–∞–Ω–∏–µ | –§–æ—Ä–º—É–ª–∞ | –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ | –ì–¥–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å? |
| :--- | :--- | :--- | :--- |
| **ReLU** | $max(0, x)$ | –ë—ã—Å—Ç—Ä–∞—è, —Ä–µ—à–∞–µ—Ç –∑–∞—Ç—É—Ö–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç. | –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç). |
| **Sigmoid** | $1 / (1 + e^{-x})$ | –í—ã—Ö–æ–¥ [0, 1]. –ó–∞—Ç—É—Ö–∞–µ—Ç –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö $x$. | –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è. |
| **Softmax** | $e^{x_i} / \sum e^{x_j}$ | –°—É–º–º–∞ –≤—ã—Ö–æ–¥–æ–≤ = 1. | –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤). |
| **Leaky ReLU**| $max(0.01x, x)$ | –ù–µ –¥–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω–∞–º "—É–º–∏—Ä–∞—Ç—å". | –ï—Å–ª–∏ –æ–±—ã—á–Ω—ã–π ReLU –Ω–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è. |

---

## üìâ –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Loss Functions)
–û—à–∏–±–∫–∞ ‚Äî —ç—Ç–æ "–∫–æ–º–ø–∞—Å" –¥–ª—è –º–æ–¥–µ–ª–∏. –û–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º—ã –¥–∞–ª–µ–∫–æ –æ—Ç –∏—Å—Ç–∏–Ω—ã.

1.  **MSE (Mean Squared Error)**: –°—Ä–µ–¥–Ω–∏–π –∫–≤–∞–¥—Ä–∞—Ç —Ä–∞–∑–Ω–∏—Ü—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ **—Ä–µ–≥—Ä–µ—Å—Å–∏–∏**. –°–∏–ª—å–Ω–æ –Ω–∞–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞ –±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏.
2.  **Binary Cross-Entropy (Log Loss)**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è **–±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**. –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.
3.  **Categorical Cross-Entropy**: –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤.

---

## üîÑ –û–±—É—á–µ–Ω–∏–µ: Backpropagation –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã

–û–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –ø–æ–∏—Å–∫ —Ç–∞–∫–∏—Ö –≤–µ—Å–æ–≤, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö Loss –º–∏–Ω–∏–º–∞–ª–µ–Ω.

### Backpropagation (–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ)
–í –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç **–¶–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ (Chain Rule)** –∏–∑ –º–∞—Ç–∞–Ω–∞–ª–∏–∑–∞. –ú—ã —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é –æ—à–∏–±–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –≤–µ—Å—É:
$$\frac{\partial Loss}{\partial w} = \frac{\partial Loss}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$$
–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–Ω—è—Ç—å: "–ï—Å–ª–∏ —è –∏–∑–º–µ–Ω—é —ç—Ç–æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–µ—Å –Ω–∞ 0.001, –∫–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—Å—è –∏—Ç–æ–≥–æ–≤–∞—è –æ—à–∏–±–∫–∞?".

### –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (Gradient Descent Variants)
- **SGD (Stochastic Gradient Descent)**: –ë–∞–∑–æ–≤—ã–π —Å–ø—É—Å–∫. –ë—ã—Å—Ç—Ä—ã–π, –Ω–æ —à—É–º–Ω—ã–π.
- **Momentum**: –î–æ–±–∞–≤–ª—è–µ—Ç "–∏–Ω–µ—Ä—Ü–∏—é". –ï—Å–ª–∏ –º—ã –¥–æ–ª–≥–æ –∏–¥–µ–º –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É, –º—ã —É—Å–∫–æ—Ä—è–µ–º—Å—è. –ü–æ–º–æ–≥–∞–µ—Ç –ø—Ä–æ—Å–∫–∞–∫–∏–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã.
- **Adam (Adaptive Moment Estimation)**: –ö–æ—Ä–æ–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤. –°–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –∏–¥–µ–∏ Momentum –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π Learning Rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Å–µ–≥–¥–∞ —Å—Ç–æ–∏—Ç –Ω–∞—á–∏–Ω–∞—Ç—å —Å –Ω–µ–≥–æ.

---

## üõ°Ô∏è –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: –ë–æ—Ä—å–±–∞ —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º

1.  **Dropout**: –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º—ã —Å–ª—É—á–∞–π–Ω–æ "–≤—ã–∫–ª—é—á–∞–µ–º" (–∑–∞–Ω—É–ª—è–µ–º) —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 20%). –≠—Ç–æ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ç—å –Ω–µ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–≤—è–∑–∏ –∏ –±—ã—Ç—å –±–æ–ª–µ–µ "–∂–∏–≤—É—á–µ–π".
2.  **L1 / L2 Regularization**: –ú—ã –¥–æ–±–∞–≤–ª—è–µ–º –∫ Loss —Ñ—É–Ω–∫—Ü–∏–∏ —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞. –≠—Ç–æ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—Å–∞ –±—ã—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –∏ –ø—Ä–æ—Å—Ç—ã–º–∏.
3.  **Early Stopping**: –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ –æ—à–∏–±–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Ç–∏, –∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –≤—Å—ë –µ—â–µ –ø–∞–¥–∞–µ—Ç.

---

## üèóÔ∏è –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

### Batch Normalization
–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö **–≤–Ω—É—Ç—Ä–∏** —Å–µ—Ç–∏ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏.
- –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–∞–∑—ã.
- –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π Learning Rate.
- –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ª–µ–≥–∫–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è.

### Weight Initialization
–ï—Å–ª–∏ –∑–∞–¥–∞—Ç—å –≤—Å–µ –≤–µ—Å–∞ –Ω—É–ª—è–º–∏ ‚Äî —Å–µ—Ç—å –Ω–µ –±—É–¥–µ—Ç —É—á–∏—Ç—å—Å—è.
- **Xavier/Glorot**: –î–ª—è Sigmoid/Tanh.
- **He Initialization**: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è **ReLU** (—Å—Ç–∞–Ω–¥–∞—Ä—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏).

---

> [!IMPORTANT]
> **Vanishing Gradient (–ó–∞—Ç—É—Ö–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç)**: –í –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Sigmoid –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø—Ä–∏ —É–º–Ω–æ–∂–µ–Ω–∏–∏ (Chain Rule) —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å—ë –º–µ–Ω—å—à–µ –∏ –º–µ–Ω—å—à–µ, –ø–æ–∫–∞ –Ω–µ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ 0. –í –∏—Ç–æ–≥–µ –ø–µ—Ä–≤—ã–µ —Å–ª–æ–∏ –≤–æ–æ–±—â–µ –Ω–µ —É—á–∞—Ç—Å—è. –†–µ—à–µ–Ω–∏–µ ‚Äî **ReLU** –∏ **Batch Norm**.

---

> [!TIP]
> –ï—Å–ª–∏ –≤–∞—à–∞ –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ —É—á–∏—Ç—Å—è:
> 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Learning Rate (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ 1e-3).
> 2. –î–æ–±–∞–≤—å—Ç–µ Batch Normalization.
> 3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ –ø–æ–ø—Ä–æ—â–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. üõ†Ô∏è
