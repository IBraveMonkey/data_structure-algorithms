# ๐ง ะะตะนัะพะฝะฝัะต ัะตัะธ (Neural Networks)

## ๐ ะกะพะดะตัะถะฐะฝะธะต
1. [ะงัะพ ัะฐะบะพะต ะฝะตะนัะพะฝะฝะฐั ัะตัั?](#ััะพ-ัะฐะบะพะต-ะฝะตะนัะพะฝะฝะฐั-ัะตัั)
2. [ะะตััะตะฟััะพะฝ: ะัะพััะตะนัะธะน ะฝะตะนัะพะฝ](#ะฟะตััะตะฟััะพะฝ)
3. [ะคัะฝะบัะธะธ ะฐะบัะธะฒะฐัะธะธ](#ััะฝะบัะธะธ-ะฐะบัะธะฒะฐัะธะธ)
4. [ะััะธัะตะบัััะฐ ัะตัะธ](#ะฐััะธัะตะบัััะฐ-ัะตัะธ)
5. [ะะฑัะฐัะฝะพะต ัะฐัะฟัะพัััะฐะฝะตะฝะธะต (Backpropagation)](#backpropagation)
6. [ะัะฐะดะธะตะฝัะฝัะน ัะฟััะบ (Gradient Descent)](#gradient-descent)

---

## โ ะงัะพ ัะฐะบะพะต ะฝะตะนัะพะฝะฝะฐั ัะตัั?

**ะะตะนัะพะฝะฝะฐั ัะตัั** โ ััะพ ะผะฐัะตะผะฐัะธัะตัะบะฐั ะผะพะดะตะปั, ะฒะดะพัะฝะพะฒะปะตะฝะฝะฐั ัะฐะฑะพัะพะน ะฑะธะพะปะพะณะธัะตัะบะธั ะฝะตะนัะพะฝะพะฒ ะฒ ะผะพะทะณะต. ะะฝะฐ ัะพััะพะธั ะธะท ะผะฝะพะถะตััะฒะฐ ะฟัะพัััั ัะปะตะผะตะฝัะพะฒ (ะฝะตะนัะพะฝะพะฒ), ะบะพัะพััะต ัะพะตะดะธะฝะตะฝั ะผะตะถะดั ัะพะฑะพะน ะธ ะพะฑัะฐะฑะฐััะฒะฐัั ะธะฝัะพัะผะฐัะธั ะฟะฐัะฐะปะปะตะปัะฝะพ. ๐งฌ

> [!NOTE]
> ะ ัะตะฐะปัะฝะพะผ ะผะพะทะณะต ~86 ะผะธะปะปะธะฐัะดะพะฒ ะฝะตะนัะพะฝะพะฒ. ะ ะธัะบััััะฒะตะฝะฝัั ัะตััั โ ะพั ัะพัะตะฝ ะดะพ ะผะธะปะปะธะฐัะดะพะฒ ะฟะฐัะฐะผะตััะพะฒ (ะฒะตัะพะฒ).

---

## โก ะะตััะตะฟััะพะฝ: ะัะพััะตะนัะธะน ะฝะตะนัะพะฝ

**ะะตััะตะฟััะพะฝ** โ ััะพ ะฑะฐะทะพะฒัะน ัััะพะธัะตะปัะฝัะน ะฑะปะพะบ ะฝะตะนัะพะฝะฝะพะน ัะตัะธ. ะะฝ ะฟัะธะฝะธะผะฐะตั ะฝะตัะบะพะปัะบะพ ะฒัะพะดะพะฒ, ัะผะฝะพะถะฐะตั ะบะฐะถะดัะน ะฝะฐ ะฒะตั, ััะผะผะธััะตั ะธ ะฟัะพะฟััะบะฐะตั ัะตัะตะท ััะฝะบัะธั ะฐะบัะธะฒะฐัะธะธ.

```mermaid
graph LR
    X1[x1] -- w1 --> S((ฮฃ))
    X2[x2] -- w2 --> S
    X3[x3] -- w3 --> S
    B[bias] --> S
    S --> A[Activation f]
    A --> Y[Output]
```

**ะคะพัะผัะปะฐ:**
```
output = activation(w1*x1 + w2*x2 + w3*x3 + bias)
```

- **ะะตัะฐ (w)**: ะะฟัะตะดะตะปััั ะฒะฐะถะฝะพััั ะบะฐะถะดะพะณะพ ะฒัะพะดะฐ.
- **Bias (ัะผะตัะตะฝะธะต)**: ะะพะทะฒะพะปัะตั ัะดะฒะธะณะฐัั ััะฝะบัะธั ะฐะบัะธะฒะฐัะธะธ.

---

## ๐ ะคัะฝะบัะธะธ ะฐะบัะธะฒะฐัะธะธ

ะคัะฝะบัะธะธ ะฐะบัะธะฒะฐัะธะธ ะดะตะปะฐัั ะฝะตะนัะพะฝะฝัั ัะตัั **ะฝะตะปะธะฝะตะนะฝะพะน**. ะะตะท ะฝะธั ะดะฐะถะต 100-ัะปะพะนะฝะฐั ัะตัั ะฑัะปะฐ ะฑั ัะบะฒะธะฒะฐะปะตะฝัะฝะฐ ะพะดะฝะพะผั ัะปะพั (ะปะธะฝะตะนะฝะฐั ะบะพะผะฑะธะฝะฐัะธั ะปะธะฝะตะนะฝัั ััะฝะบัะธะน = ะปะธะฝะตะนะฝะฐั ััะฝะบัะธั).

### 1. Sigmoid (ะกะธะณะผะพะธะดะฐ) ๐
```
ฯ(x) = 1 / (1 + e^(-x))
```
- **ะะธะฐะฟะฐะทะพะฝ**: [0, 1]
- **ะะปััั**: ะะปะฐะดะบะฐั, ะธะฝัะตัะฟัะตัะธััะตััั ะบะฐะบ ะฒะตัะพััะฝะพััั.
- **ะะธะฝััั**: ะัะพะฑะปะตะผะฐ ะทะฐัััะฐััะตะณะพ ะณัะฐะดะธะตะฝัะฐ (vanishing gradient) ะฟัะธ ะฑะพะปััะธั/ะผะฐะปัั x.

---

### 2. Tanh (ะะธะฟะตัะฑะพะปะธัะตัะบะธะน ัะฐะฝะณะตะฝั) ใฐ๏ธ
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
- **ะะธะฐะฟะฐะทะพะฝ**: [-1, 1]
- **ะะปััั**: ะฆะตะฝััะธัะพะฒะฐะฝะฐ ะฒะพะบััะณ ะฝัะปั (ะปัััะต ะดะปั ัะบััััั ัะปะพะตะฒ).
- **ะะธะฝััั**: ะขะฐะบะถะต ัััะฐะดะฐะตั ะพั ะทะฐัััะฐััะตะณะพ ะณัะฐะดะธะตะฝัะฐ.

---

### 3. ReLU (Rectified Linear Unit) โก
```
ReLU(x) = max(0, x)
```
- **ะะธะฐะฟะฐะทะพะฝ**: [0, โ)
- **ะะปััั**: ะัะพััะฐั, ะฑััััะฐั, ัะตัะฐะตั ะฟัะพะฑะปะตะผั ะทะฐัััะฐััะตะณะพ ะณัะฐะดะธะตะฝัะฐ, ะดะต-ัะฐะบัะพ ััะฐะฝะดะฐัั.
- **ะะธะฝััั**: "ะะตััะฒัะต ะฝะตะนัะพะฝั" (ะตัะปะธ ะฒะตั ััะฐะป ะพััะธัะฐัะตะปัะฝัะผ, ะณัะฐะดะธะตะฝั = 0 ะฝะฐะฒัะตะณะดะฐ).

---

### 4. Leaky ReLU ๐ง
```
Leaky ReLU(x) = max(0.01x, x)
```
- ะะตัะฐะตั ะฟัะพะฑะปะตะผั "ะผะตััะฒัั ะฝะตะนัะพะฝะพะฒ", ะฟะพะทะฒะพะปัั ะผะฐะปัะน ะพััะธัะฐัะตะปัะฝัะน ะณัะฐะดะธะตะฝั.

---

### 5. Softmax (ะดะปั ะบะปะฐััะธัะธะบะฐัะธะธ) ๐ฏ
ะัะตะฒัะฐัะฐะตั ะฒะตะบัะพั ัะธัะตะป ะฒ ัะฐัะฟัะตะดะตะปะตะฝะธะต ะฒะตัะพััะฝะพััะตะน (ััะผะผะฐ = 1).
```
softmax(xi) = e^xi / ฮฃ(e^xj)
```
ะัะฟะพะปัะทัะตััั ะฒ ะฟะพัะปะตะดะฝะตะผ ัะปะพะต ะดะปั ะผะฝะพะณะพะบะปะฐััะพะฒะพะน ะบะปะฐััะธัะธะบะฐัะธะธ.

---

## ๐๏ธ ะััะธัะตะบัััะฐ ัะตัะธ

ะะตะนัะพะฝะฝะฐั ัะตัั ัะพััะพะธั ะธะท **ัะปะพะตะฒ** (layers):

1.  **ะัะพะดะฝะพะน ัะปะพะน (Input Layer)**: ะัะธะฝะธะผะฐะตั ะดะฐะฝะฝัะต (ะฝะฐะฟัะธะผะตั, ะฟะธะบัะตะปะธ ะบะฐััะธะฝะบะธ).
2.  **ะกะบััััะต ัะปะพะธ (Hidden Layers)**: ะะทะฒะปะตะบะฐัั ะฟัะธะทะฝะฐะบะธ ะฒััะพะบะพะณะพ ััะพะฒะฝั. ะงะตะผ ะฑะพะปััะต ัะปะพะตะฒ, ัะตะผ ัะปะพะถะฝะตะต ะฟะฐััะตัะฝั ัะตัั ะผะพะถะตั ัะปะพะฒะธัั.
3.  **ะััะพะดะฝะพะน ัะปะพะน (Output Layer)**: ะัะดะฐะตั ะฟัะตะดัะบะฐะทะฐะฝะธะต.

```mermaid
graph LR
    I1[Input 1] --> H1[Hidden 1]
    I2[Input 2] --> H1
    I3[Input 3] --> H1
    
    I1 --> H2[Hidden 2]
    I2 --> H2
    I3 --> H2
    
    H1 --> O1[Output 1]
    H2 --> O1
    H1 --> O2[Output 2]
    H2 --> O2
    
    style I1 fill:#9cf
    style I2 fill:#9cf
    style I3 fill:#9cf
    style O1 fill:#f9c
    style O2 fill:#f9c
```

> [!IMPORTANT]
> **ะะปัะฑะพะบะฐั ัะตัั (Deep Network)** โ ััะพ ัะตัั ั ะผะฝะพะถะตััะฒะพะผ ัะบััััั ัะปะพะตะฒ (ะพะฑััะฝะพ > 3). ะัััะดะฐ ัะตัะผะธะฝ **Deep Learning**.

---

## ๐ ะะฑัะฐัะฝะพะต ัะฐัะฟัะพัััะฐะฝะตะฝะธะต (Backpropagation)

ะญัะพ ะฐะปะณะพัะธัะผ ะพะฑััะตะฝะธั ะฝะตะนัะพะฝะฝะพะน ัะตัะธ. ะะฝ ัะฐะฑะพัะฐะตั ะฒ ะดะฒะฐ ััะฐะฟะฐ:

### 1. Forward Pass (ะััะผะพะน ะฟัะพัะพะด)
ะะฐะฝะฝัะต ะฟัะพัะพะดัั ัะตัะตะท ัะตัั ะพั ะฒัะพะดะฐ ะบ ะฒััะพะดั. ะะพะปััะฐะตะผ ะฟัะตะดัะบะฐะทะฐะฝะธะต.

### 2. Backward Pass (ะะฑัะฐัะฝัะน ะฟัะพัะพะด)
ะกัะธัะฐะตะผ ะพัะธะฑะบั (loss) ะธ ัะฐัะฟัะพัััะฐะฝัะตะผ ะตั ะพะฑัะฐัะฝะพ ัะตัะตะท ัะตัั, ะพะฑะฝะพะฒะปัั ะฒะตัะฐ.

```mermaid
graph LR
    Input[ะัะพะดะฝัะต ะดะฐะฝะฝัะต] --> FP[Forward Pass]
    FP --> Pred[ะัะตะดัะบะฐะทะฐะฝะธะต]
    Pred --> L[Loss ััะฝะบัะธั]
    True[ะััะธะฝะฝะพะต ะทะฝะฐัะตะฝะธะต] --> L
    L --> BP[Backpropagation]
    BP --> UW[ะะฑะฝะพะฒะปะตะฝะธะต ะฒะตัะพะฒ]
    UW --> Input
```

**ะะปััะตะฒะฐั ะธะดะตั**: ะัะฟะพะปัะทัะตััั **ะฟัะฐะฒะธะปะพ ัะตะฟะพัะบะธ (chain rule)** ะธะท ะผะฐัะตะผะฐัะธัะตัะบะพะณะพ ะฐะฝะฐะปะธะทะฐ, ััะพะฑั ะฟะพะฝััั, ะบะฐะบ ะธะทะผะตะฝะตะฝะธะต ะบะฐะถะดะพะณะพ ะฒะตัะฐ ะฒะปะธัะตั ะฝะฐ ะธัะพะณะพะฒัั ะพัะธะฑะบั.

---

## โฐ๏ธ ะัะฐะดะธะตะฝัะฝัะน ัะฟััะบ (Gradient Descent)

ะญัะพ ะผะตัะพะด ะพะฟัะธะผะธะทะฐัะธะธ, ะบะพัะพััะน ะผะธะฝะธะผะธะทะธััะตั ััะฝะบัะธั ะฟะพัะตัั (loss function).

**ะะปะณะพัะธัะผ:**
1. ะััะธัะปะธัั ะณัะฐะดะธะตะฝั (ะฟัะพะธะทะฒะพะดะฝัั) ััะฝะบัะธะธ ะฟะพัะตัั ะฟะพ ะบะฐะถะดะพะผั ะฒะตัั.
2. ะกะดะฒะธะฝััั ะฒะตั ะฒ ะฝะฐะฟัะฐะฒะปะตะฝะธะธ, ะฟัะพัะธะฒะพะฟะพะปะพะถะฝะพะผ ะณัะฐะดะธะตะฝัั.
3. ะะพะฒัะพัััั, ะฟะพะบะฐ ะฝะต ะดะพััะธะณะฝะตะผ ะผะธะฝะธะผัะผะฐ.

```
ะฝะพะฒัะน_ะฒะตั = ััะฐััะน_ะฒะตั - learning_rate * ะณัะฐะดะธะตะฝั
```

---

### ะะฐัะธะฐะฝัั:
- **Batch GD**: ะกัะธัะฐะตั ะณัะฐะดะธะตะฝั ะฟะพ ะฒัะตะผ ะดะฐะฝะฝัะผ (ะผะตะดะปะตะฝะฝะพ, ะฝะพ ััะฐะฑะธะปัะฝะพ).
- **Stochastic GD (SGD)**: ะกัะธัะฐะตั ะณัะฐะดะธะตะฝั ะฟะพ ะพะดะฝะพะผั ะฟัะธะผะตัั (ะฑััััะพ, ะฝะพ ััะผะฝะพ).
- **Mini-batch GD**: ะะพะปะพัะฐั ัะตัะตะดะธะฝะฐ โ ะฑะตัะตั ะฝะตะฑะพะปัััั ะฟะพััะธั ะดะฐะฝะฝัั (batch size = 32, 64, 128).

---

### ะัะพะดะฒะธะฝัััะต ะพะฟัะธะผะธะทะฐัะพัั:
- **Adam**: ะกะฐะผัะน ะฟะพะฟัะปััะฝัะน. ะะดะฐะฟัะธะฒะฝะพ ะผะตะฝัะตั learning rate ะดะปั ะบะฐะถะดะพะณะพ ะฟะฐัะฐะผะตััะฐ.
- **RMSprop**: ะฅะพัะพั ะดะปั ัะตะบัััะตะฝัะฝัั ัะตัะตะน.
- **AdaGrad**: ะะปั ัะฐะทัะตะถะตะฝะฝัั ะดะฐะฝะฝัั.

---

> [!TIP]
> **ะะธะฟะตัะฟะฐัะฐะผะตััั, ะบะพัะพััะต ะฝัะถะฝะพ ะฝะฐัััะฐะธะฒะฐัั:**
> - Learning rate (ัะปะธัะบะพะผ ะฑะพะปััะพะน โ ะฝะต ััะพะดะธััั, ัะปะธัะบะพะผ ะผะฐะปัะน โ ััะธััั ะฒะตัะฝะพ).
> - Batch size.
> - ะะพะปะธัะตััะฒะพ ัะปะพะตะฒ ะธ ะฝะตะนัะพะฝะพะฒ ะฒ ะบะฐะถะดะพะผ.
> - ะคัะฝะบัะธั ะฐะบัะธะฒะฐัะธะธ.
> 
> ะะฑััะฝะพ ััะพ ะดะตะปะฐะตััั ัะตัะตะท **grid search** ะธะปะธ **random search**. ๐ฏ
